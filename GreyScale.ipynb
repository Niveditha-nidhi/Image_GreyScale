{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZuXikxjoW1H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import savefig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_daisy = 'flower_photos/daisy/*.jpg'\n",
        "path_dandelion = 'flower_photos/dandelion/*.jpg'\n",
        "path_roses = 'flower_photos/roses/*.jpg'\n",
        "path_sunflowers = 'flower_photos/sunflowers/*.jpg'\n",
        "path_tulips = 'flower_photos/tulipaths_list = [path_daisy, path_dandelion, path_roses, path_sunflowers, path_tulips]ps/*.jpg'\n"
      ],
      "metadata": {
        "id": "zYmHGkYtqfry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 1\n",
        "c = 0\n",
        "for path in paths_list:\n",
        "    filenames = glob.glob(path)\n",
        "    for filename in filenames:\n",
        "        image = cv2.imread(filename)\n",
        "        gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = cv2.resize(image, (128, 128))\n",
        "        gray_img = cv2.resize(gray_img, (128, 128))\n",
        "        cv2.imwrite(\"gray_images/gray_\" +str(count) +\".jpg\", gray_img)\n",
        "        cv2.imwrite(\"color_images/color_\" +str(count) +\".jpg\", image)\n",
        "        count += 1\n",
        "        c+=1\n"
      ],
      "metadata": {
        "id": "T-753tqnrC8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = glob.glob(\"flower_images\\*.png\")\n",
        "count = 1\n",
        "for file in filenames:\n",
        "    img = cv2.imread(file, 0)\n",
        "    img = cv2.resize(img, (128, 128))\n",
        "    cv2.imwrite('actual_gray_test/gray_' +str(count) +'.jpg', img)\n",
        "    count += 1\n"
      ],
      "metadata": {
        "id": "IV7lwpasrNPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "#Read all color images and append into numpy list\n",
        "for i in range(1, c+1):\n",
        "    img = cv2.imread(\"color_images/color_\" +str(i) +\".jpg\" )\n",
        "    dataset.append(np.array(img))\n",
        "\n",
        "dataset_source = np.asarray(dataset)\n",
        "print(dataset_source.shape)\n",
        "\n",
        "dataset_tar = []\n",
        "\n",
        "#Read all grayscale images and append into numpy list\n",
        "for i in range(1, c+1):\n",
        "    img = cv2.imread(\"gray_images/gray_\" +str(i) +\".jpg\", 0)\n",
        "    dataset_tar.append(np.array(img))\n",
        "\n",
        "dataset_target = np.asarray(dataset_tar)\n",
        "print(dataset_target.shape)\n"
      ],
      "metadata": {
        "id": "bTgvOs-9rOWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b8cd81-1e54-48bd-e672-92a2327a4965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,)\n",
            "(0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(inputs):\n",
        "\n",
        "    # Encoder\n",
        "    net = tf.layers.conv2d(inputs, 128, 2, activation = tf.nn.relu)\n",
        "    net = tf.layers.max_pooling2d(net, 2, 2, padding = 'same')\n",
        "\n",
        "    # Decoder\n",
        "    net = tf.image.resize_nearest_neighbor(net, tf.constant([129, 129]))\n",
        "    net = tf.layers.conv2d(net, 1, 2, activation = None, name = 'outputOfAuto')\n",
        "\n",
        "    return net\n"
      ],
      "metadata": {
        "id": "dBh2xC6drSoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_iEC1Cayk7p",
        "outputId": "cdded9ca-5549-44d2-ce2f-38ad60f78ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow_addons.optimizers import AdamW\n",
        "\n",
        "# Define your autoencoder function using Keras layers\n",
        "def autoencoder(inputs):\n",
        "    # Encoder\n",
        "    net = Conv2D(128, (2, 2), activation=tf.nn.relu)(inputs)\n",
        "    net = MaxPooling2D((2, 2), padding='same')(net)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(net)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "ae_inputs = tf.keras.Input(shape=(256, 256, 3), name='inputToAuto', dtype=tf.float32)\n",
        "ae_target_resized = tf.image.resize(ae_target, (256, 256))  # Resize target to match output size\n",
        "ae_outputs = autoencoder(ae_inputs)\n",
        "lr = 0.001 # learning rate\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(ae_outputs - ae_target))\n",
        "train_op = AdamW(learning_rate=lr, weight_decay=0.001).minimize(loss, var_list=[ae_inputs, ae_outputs])\n",
        "init = tf.global_variables_initializer()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VTI4MRhWyqxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow_addons.optimizers import AdamW\n",
        "\n",
        "# Define your autoencoder function using Keras layers\n",
        "def autoencoder(inputs):\n",
        "    # Encoder\n",
        "    net = Conv2D(128, (2, 2), activation=tf.nn.relu)(inputs)\n",
        "    net = MaxPooling2D((2, 2), padding='same')(net)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(net)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "ae_inputs = tf.keras.Input(shape=(256, 256, 3), name='inputToAuto', dtype=tf.float32)\n",
        "ae_target_resized = tf.image.resize(ae_target, (256, 256))  # Resize target to match output size\n",
        "ae_outputs = autoencoder(ae_inputs)\n",
        "lr = 0.001 # learning rate\n",
        "\n",
        "loss = tf.convert_to_tensor(tf.reduce_mean(tf.square(ae_outputs - ae_target)))\n",
        "train_op = AdamW(learning_rate=lr, weight_decay=0.001).minimize(tf.cast(loss, tf.float32), var_list=[ae_inputs, ae_outputs])\n",
        "init = tf.global_variables_initializer()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gVDXbIqrzCk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the constant variables\n",
        "batch_size = 32\n",
        "epoch_num = 50\n",
        "saving_path = 'K:/autoencoder_color_to_gray/SavedModel/AutoencoderColorToGray.ckpt'\n",
        "saver_ = tf.train.Saver(max_to_keep = 3)\n",
        "\n",
        "# input data to the network is stored in variables\n",
        "batch_img = dataset_source[0:batch_size]\n",
        "batch_out = dataset_target[0:batch_size]\n",
        "num_batches = num_images//batch_size\n",
        "\n",
        "# Create a session object and run the global variable which was defined earlier\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "# new images are sent into the network in batches of 32\n",
        "for ep in range(epoch_num):\n",
        "    batch_size = 0\n",
        "    for batch_n in range(num_batches): # batches loop\n",
        "\n",
        "        # runs the computational graph in the autoencoder with the given input data and target data\n",
        "        _, c = sess.run([train_op, loss], feed_dict = {ae_inputs: batch_img, ae_target: batch_out})\n",
        "        print(\"Epoch: {} - cost = {:.5f}\" .format((ep+1), c))\n",
        "        batch_img = dataset_source[batch_size: batch_size+32]\n",
        "        batch_out = dataset_target[batch_size: batch_size+32]\n",
        "        batch_size += 32\n",
        "\n",
        "    saver_.save(sess, saving_path, global_step = ep)\n",
        "\n",
        "sess.close()\n"
      ],
      "metadata": {
        "id": "rnMURZYErbRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "saver.restore(sess, 'K:/autoencoder_color_to_gray/SavedModel/AutoencoderColorToGray.ckpt-49')\n",
        "\n",
        "test_data = []\n",
        "for file in filenames[0:100]:\n",
        "    test_data.append(np.array(cv2.imread(file)))\n",
        "\n",
        "test_dataset = np.asarray(test_data)\n",
        "batch_imgs = test_dataset\n",
        "gray_imgs = sess.run(ae_outputs, feed_dict = {ae_inputs: batch_imgs})\n",
        "\n",
        "for i in range(gray_imgs.shape[0]):\n",
        "    cv2.imwrite('gen_gray_images/gen_gray_' +str(i) +'.jpeg', gray_imgs[i])\n"
      ],
      "metadata": {
        "id": "TElmF4PcrgS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}